{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import models, layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import pandas\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emoji(tweet):\n",
    "    x = re.compile(r'[^a-z\\s]').sub(r'', re.compile(r'[\\W]').sub(r' ', tweet.lower()))\n",
    "    x = tokenizer.texts_to_sequences(np.array([x]))\n",
    "    x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=max_length)\n",
    "    result = model.predict(x)\n",
    "    print(\"'\"+tweet+\"' got the emoji: \",  np.argmax(result, axis=1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42627, 4)\n",
      "(10657, 4)\n",
      "                                               tweet  emoji_class emoji  \\\n",
      "0  Brought to you courtesy of the red white and b...            5    ðŸ‡ºðŸ‡¸   \n",
      "1                 @user #taotuesday @ TAO Downtown\\n            3     ðŸ”¥   \n",
      "2  Ready to celebrate America with @user Happy #i...            5    ðŸ‡ºðŸ‡¸   \n",
      "3  10min project w old footage #houstonphotograph...            2     ðŸ“¸   \n",
      "4  Usually I don't put song on my insta but this ...            3     ðŸ”¥   \n",
      "\n",
      "  predicted_class  \n",
      "0            None  \n",
      "1            None  \n",
      "2            None  \n",
      "3            None  \n",
      "4            None  \n"
     ]
    }
   ],
   "source": [
    "train_tweets = pandas.read_pickle(\"emoji_train.pkl\")\n",
    "train_emojis = pandas.read_pickle(\"emoji_train.pkl\")[\"emoji_class\"]\n",
    "test_tweets = pandas.read_pickle(\"emoji_test.pkl\")\n",
    "test_emojis = pandas.read_pickle(\"emoji_test.pkl\")[\"emoji_class\"]\n",
    "print(train_tweets.shape)\n",
    "print(test_tweets.shape)\n",
    "\n",
    "print(pandas.read_pickle(\"emoji_train.pkl\").head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    bring courtesy red white blue welcome home sol...\n",
      "1                                         tao downtown\n",
      "2                                ready celebrate happy\n",
      "3                             min project  old footage\n",
      "4                     usually dont put song fire thank\n",
      "5      toilet talk radiate meditate life journey peace\n",
      "6                                          south beach\n",
      "7               red want spice red visit thats lighten\n",
      "8    attempt first  together little nervous river half\n",
      "9                                         hell kitchen\n",
      "Name: tweet, dtype: object\n",
      "0                    perfect look act like jean\n",
      "1      like call tandem decide would share love\n",
      "2                      crab dip toast miss miss\n",
      "3              happy thanksgiving family county\n",
      "4                               soho house west\n",
      "5             love trash talk thing black coach\n",
      "6                  beautiful day today downtown\n",
      "7      go ignore fact look like video miss much\n",
      "8    run little man today huge help family love\n",
      "9                              tree dress merry\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def cleanUpTweet(txt):\n",
    "    # Remove mentions (target)\n",
    "    txt = re.sub(r'@[A-Za-z0-9_]+', '', txt)\n",
    "    # Remove hashtags\n",
    "    txt = re.sub(r'#', '', txt)\n",
    "    # Remove retweets:\n",
    "    txt = re.sub(r'RT : ', '', txt)\n",
    "    # Remove urls\n",
    "    txt = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', txt)\n",
    "    return txt\n",
    "def removePunSymNum(txt):\n",
    "    #remove alle characters except a-zA-Z\n",
    "    txt = re.sub(r'([^a-zA-Z ])', '', txt) #regex expression validated with https://regexr.com/\n",
    "    # Removing additional whitespaces\n",
    "    txt = re.sub(r\" +\", ' ', txt)\n",
    "    return txt\n",
    "def normalize(text):\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = word_tokenize(text,language='english')\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if not word in stopword_set]  \n",
    "    return text\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text)\n",
    "    for word in text:\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return corrected_text\n",
    "\n",
    "def pos_tagging(text):\n",
    "    return nltk.pos_tag(text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tag_dict = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "\n",
    "def lemmatize_words(pos_tagged_text):\n",
    "    return [lemmatizer.lemmatize(word, tag_dict.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text]\n",
    "\n",
    "en_words_set = set(nltk.corpus.words.words())\n",
    "\n",
    "def remove_none_en_word(text):\n",
    "    return [word for word in text if word in en_words_set]\n",
    "\n",
    "def clean_single_char(text):\n",
    "    return [re.sub(r'(^| ).(( ).)*( |$)','', word) for word in text] \n",
    "\n",
    "def back_to_sentence(text):\n",
    "    return \" \".join(text) \n",
    "\n",
    "train_tweets_copy = train_tweets.copy()\n",
    "test_tweets_copy = test_tweets.copy()\n",
    "\n",
    "train_tweets_copy['tweet'] = train_tweets['tweet'] \\\n",
    "                        .apply(cleanUpTweet) \\\n",
    "                        .apply(removePunSymNum) \\\n",
    "                        .apply(normalize) \\\n",
    "                        .apply(tokenize) \\\n",
    "                        .apply(remove_stopwords) \\\n",
    "                        .apply(pos_tagging) \\\n",
    "                        .apply(lemmatize_words) \\\n",
    "                        .apply(remove_none_en_word) \\\n",
    "                        .apply(clean_single_char) \\\n",
    "                        .apply(back_to_sentence)\n",
    "\n",
    "test_tweets_copy['tweet'] = test_tweets['tweet'] \\\n",
    "                        .apply(cleanUpTweet) \\\n",
    "                        .apply(removePunSymNum) \\\n",
    "                        .apply(normalize) \\\n",
    "                        .apply(tokenize) \\\n",
    "                        .apply(remove_stopwords) \\\n",
    "                        .apply(pos_tagging) \\\n",
    "                        .apply(lemmatize_words) \\\n",
    "                        .apply(remove_none_en_word) \\\n",
    "                        .apply(clean_single_char) \\\n",
    "                        .apply(back_to_sentence)\n",
    "\"\"\" \n",
    "train_tweets_copy['tweet'] = train_tweets['tweet'] \\\n",
    "                        .apply(cleanUpTweet) \\\n",
    "                        .apply(removePunSymNum) \\\n",
    "                        .apply(normalize) \\\n",
    "                        .apply(tokenize) \\\n",
    "                        .apply(remove_stopwords) \\\n",
    "                        .apply(correct_spellings) \\\n",
    "                        .apply(pos_tagging) \\\n",
    "                        .apply(lemmatize_words) \\\n",
    "                        .apply(remove_none_en_word) \\\n",
    "                        .apply(clean_single_char)\n",
    "                        .apply(back_to_sentence)\n",
    "\"\"\"\n",
    "\n",
    "print(train_tweets_copy.head(10)[\"tweet\"])\n",
    "print(test_tweets_copy.head(10)[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('love', 3787),\n",
       "  ('new', 2132),\n",
       "  ('get', 2109),\n",
       "  ('day', 2085),\n",
       "  ('happy', 2074),\n",
       "  ('beach', 1418),\n",
       "  ('time', 1386),\n",
       "  ('night', 1293),\n",
       "  ('one', 1187),\n",
       "  ('la', 1170),\n",
       "  ('today', 1164),\n",
       "  ('go', 1119),\n",
       "  ('good', 1117),\n",
       "  ('like', 1093),\n",
       "  ('park', 1077),\n",
       "  ('san', 1056),\n",
       "  ('family', 1014),\n",
       "  ('make', 1014),\n",
       "  ('last', 976),\n",
       "  ('come', 970)],\n",
       " [('radiate', 1),\n",
       "  ('worthless', 1),\n",
       "  ('enrol', 1),\n",
       "  ('slink', 1),\n",
       "  ('hogan', 1),\n",
       "  ('artillery', 1),\n",
       "  ('purslane', 1),\n",
       "  ('whiz', 1),\n",
       "  ('erika', 1),\n",
       "  ('deteriorate', 1),\n",
       "  ('deceitful', 1),\n",
       "  ('anticrepuscular', 1),\n",
       "  ('woodside', 1),\n",
       "  ('priory', 1),\n",
       "  ('printed', 1),\n",
       "  ('desi', 1),\n",
       "  ('historicalness', 1),\n",
       "  ('welfare', 1),\n",
       "  ('distract', 1),\n",
       "  ('drench', 1)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_words = list()\n",
    "for i in range(len(train_tweets_copy)):\n",
    "    for k in  train_tweets_copy.tweet[i].split():\n",
    "        li_words.append(k)\n",
    "\n",
    "word_cnt = Counter(li_words)\n",
    "dict_cnt_words = dict(word_cnt)\n",
    "dict_sorted_most = sorted(dict_cnt_words.items(),key = lambda x : x[1],reverse=True)\n",
    "dict_sorted_least = sorted(dict_cnt_words.items(),key = lambda x : x[1],reverse=False)\n",
    "\n",
    "\n",
    "#output 20 most and least frequent words\n",
    "dict_sorted_most[0:20],dict_sorted_least[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = train_tweets_copy[\"tweet\"].copy()\n",
    "test_tweets = test_tweets_copy[\"tweet\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#clean data\n",
    "for i in range(len(train_tweets)):\n",
    "    train_tweets[i] = re.compile(r'[^a-z0-9\\s]').sub(r'', re.compile(r'[\\W]').sub(r' ', train_tweets[i].lower()))\n",
    "for i in range(len(test_tweets)):\n",
    "    test_tweets[i] = re.compile(r'[^a-z0-9\\s]').sub(r'', re.compile(r'[\\W]').sub(r' ', test_tweets[i].lower()))\n",
    "\"\"\"\n",
    "\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_tweets)\n",
    "train_tweets = tokenizer.texts_to_sequences(train_tweets)\n",
    "test_tweets = tokenizer.texts_to_sequences(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Text preprocessing ...\")\n",
    "max_length = max(max(len(train_r) for train_r in train_tweets), max(len(train_r) for train_r in test_tweets))\n",
    "train_tweets = tf.keras.preprocessing.sequence.pad_sequences(train_tweets, maxlen=max_length)\n",
    "test_tweets = tf.keras.preprocessing.sequence.pad_sequences(test_tweets, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset ...\n",
      "\n",
      "No weights found. Training new model\n",
      "\n",
      "Training Model:\n",
      "\n",
      "Epoch 1/2\n",
      "267/267 [==============================] - 23s 77ms/step - loss: 1.6693 - accuracy: 0.4049 - val_loss: 1.3458 - val_accuracy: 0.5155\n",
      "Epoch 2/2\n",
      "267/267 [==============================] - 20s 73ms/step - loss: 1.2518 - accuracy: 0.5564 - val_loss: 1.2962 - val_accuracy: 0.5361\n",
      "\n",
      "Predictions:\n",
      "\n",
      "[[0.2602802  0.2907211  0.3418643  ... 0.02481917 0.01989494 0.02661566]\n",
      " [0.5523219  0.28275636 0.06769247 ... 0.02009727 0.01078538 0.00885693]\n",
      " [0.89823985 0.01107103 0.03667618 ... 0.00526081 0.00897477 0.00517573]\n",
      " ...\n",
      " [0.24679469 0.11362917 0.16721694 ... 0.00993871 0.01319369 0.00731013]\n",
      " [0.15840226 0.12648988 0.54765606 ... 0.0113587  0.03268366 0.02481087]\n",
      " [0.65842324 0.16152276 0.05382883 ... 0.05396543 0.01665795 0.013126  ]] \n",
      "\n",
      "\n",
      "Accuracy score: 0.5342\n",
      "\n",
      "Testing model: \n",
      "'I love this picture' got the emoji:  0\n",
      "'Look at the sunset' got the emoji:  0\n",
      "'This is so sad' got the emoji:  0\n",
      "'Bolt won the race again! First place' got the emoji:  0\n",
      "\n",
      "\n",
      "Saving model weights ...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = 'weights/emojis.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e599ebca9e6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\nSaving model weights ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weights/emojis.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\michael\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[0;32m   2105\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2106\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'h5'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2107\u001b[1;33m       \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2108\u001b[0m         \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michael\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michael\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Open in append mode (read/write).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'weights/emojis.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)"
     ]
    }
   ],
   "source": [
    "print(\"Splitting dataset ...\")\n",
    "train_tweets, validation_tweets, train_emojis, validation_emojis = train_test_split(train_tweets, train_emojis, test_size=0.2)\n",
    "\n",
    "input = layers.Input(shape=(max_length,))\n",
    "x = layers.Embedding(max_features, 128)(input)\n",
    "x = layers.LSTM(128, return_sequences=False)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "# x = layers.LSTM(128, return_sequences=False)(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(7, activation=\"softmax\")(x)\n",
    "\n",
    "model = models.Model(inputs=input, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load_weights('weights/emojis.hdf5')\n",
    "    print(\"\\nLoading previous model weights:\\n\")\n",
    "except:\n",
    "    print(\"\\nNo weights found. Training new model\\n\")\n",
    "\n",
    "print(\"Training Model:\\n\")\n",
    "model.fit(train_tweets, to_categorical(train_emojis), batch_size=128, epochs=2, validation_data=(validation_tweets, to_categorical(validation_emojis)))\n",
    "\n",
    "preds = model.predict(test_tweets)\n",
    "print(\"\\nPredictions:\\n\")\n",
    "print(preds, \"\\n\\n\")\n",
    "\n",
    "print('Accuracy score: {:0.4}'.format(accuracy_score(test_emojis, np.argmax(preds, axis=1))))\n",
    "\n",
    "print(\"\\nTesting model: \")\n",
    "\n",
    "predict_emoji(\"I love this picture\")\n",
    "predict_emoji(\"Look at the sunset\")\n",
    "predict_emoji(\"This is so sad\")\n",
    "predict_emoji(\"Bolt won the race again! First place\")\n",
    "\n",
    "print(\"\\n\\nSaving model weights ...\")\n",
    "model.save_weights('weights/emojis.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
